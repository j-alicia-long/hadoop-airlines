!	2
"".	4
"${HADOOP_WORKER_NAMES}"	1
"${JAVA_HOME}"	3
"${log}"	2
"${log}.${num}"	2
"${log}.${num}.gz"	1
"${log}.${prev}.gz"	2
"${tmpslvnames}"	1
"*"	21
"*.sink.ganglia.tagsForPrefix.rpc=port",	1
"AS	27
"ERROR:	3
"License");	27
"alice,bob	21
"clumping"	1
"console"	1
"hadoop.root.logger".	1
"hadoop_actual_ssh	1
"jks".	4
"log	1
"true"	3
#	861
#!/usr/bin/env	2
##	32
###	49
######	1
#######	1
#*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40	1
#*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both	1
#*.sink.ganglia.tagsForPrefix.dfs=HAState,IsOutOfSync	1
#*.sink.ganglia.tagsForPrefix.jvm=ProcessName	1
#*.sink.ganglia.tagsForPrefix.mapred=	1
#*.sink.ganglia.tagsForPrefix.metricssystem=*	1
#*.sink.ganglia.tagsForPrefix.rpc=port	1
#*.sink.ganglia.tagsForPrefix.rpcdetailed=port	1
#*.sink.ganglia.tagsForPrefix.ugi=*	1
#Security	1
#[docker]	1
#[fpga]	1
#crank	1
#datanode.sink.file.filename=datanode-metrics.out	1
#datanode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#datanode.webhdfs.logger=INFO,console	1
#export	14
#function	3
#jobhistoryserver.sink.file.filename=jobhistoryserver-metrics.out	1
#jobhistoryserver.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false	1
#log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=false	1
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.FSSTATEDUMP.File=${hadoop.log.dir}/fairscheduler-statedump.log	1
#log4j.appender.FSSTATEDUMP.MaxBackupIndex=${hadoop.log.maxbackupindex}	1
#log4j.appender.FSSTATEDUMP.MaxFileSize=${hadoop.log.maxfilesize}	1
#log4j.appender.FSSTATEDUMP.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.FSSTATEDUMP.layout=org.apache.log4j.PatternLayout	1
#log4j.appender.FSSTATEDUMP=org.apache.log4j.RollingFileAppender	1
#log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd	1
#log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log	1
#log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout	1
#log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender	1
#log4j.appender.HTTPDRFA.DatePattern=.yyyy-MM-dd	1
#log4j.appender.HTTPDRFA.File=${hadoop.log.dir}/hadoop-datanode-webhdfs.log	1
#log4j.appender.HTTPDRFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HTTPDRFA.layout=org.apache.log4j.PatternLayout	1
#log4j.appender.HTTPDRFA=org.apache.log4j.DailyRollingFileAppender	1
#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.amlog.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log	1
#log4j.appender.datanoderequestlog.RetainDays=3	1
#log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log	1
#log4j.appender.jobhistoryrequestlog.RetainDays=3	1
#log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log	1
#log4j.appender.namenoderequestlog.RetainDays=3	1
#log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log	1
#log4j.appender.nodemanagerrequestlog.RetainDays=3	1
#log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log	1
#log4j.appender.resourcemanagerrequestlog.RetainDays=3	1
#log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.logger.BlockStateChange=DEBUG	1
#log4j.logger.com.amazonaws=ERROR	1
#log4j.logger.datanode.webhdfs=${datanode.webhdfs.logger}	1
#log4j.logger.http.requests.datanode=INFO,datanoderequestlog	1
#log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog	1
#log4j.logger.http.requests.namenode=INFO,namenoderequestlog	1
#log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog	1
#log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog	1
#log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN	1
#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG	1
#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG	1
#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG	1
#log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}	1
#log4j.logger.org.apache.hadoop.security=DEBUG	1
#log4j.logger.org.apache.hadoop.yarn.client=DEBUG	1
#log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.statedump=DEBUG,FSSTATEDUMP	1
#log4j.logger.org.apache.hadoop.yarn.service=DEBUG	1
#mapreduce.hs.audit.logger=INFO,HSAUDIT	1
#mrappmaster.sink.file.filename=mrappmaster-metrics.out	1
#mrappmaster.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#namenode.sink.*.period=8	1
#namenode.sink.file.filename=namenode-metrics.out	1
#namenode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#nodemanager.sink.file.filename=nodemanager-metrics.out	1
#nodemanager.sink.file_jvm.class=org.apache.hadoop.metrics2.sink.FileSink	1
#nodemanager.sink.file_jvm.context=jvm	1
#nodemanager.sink.file_jvm.filename=nodemanager-jvm-metrics.out	1
#nodemanager.sink.file_mapred.class=org.apache.hadoop.metrics2.sink.FileSink	1
#nodemanager.sink.file_mapred.context=mapred	1
#nodemanager.sink.file_mapred.filename=nodemanager-mapred-metrics.out	1
#nodemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#resourcemanager.sink.file.filename=resourcemanager-metrics.out	1
#resourcemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#shellcheck	1
#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY	1
#{	3
#}	3
$HADOOP_OS_TYPE	1
$HADOOP_YARN_HOME/share/hadoop/yarn/yarn-service-examples	1
$JAVA_HOME	1
$USER	1
${HADOOP_HOME}/logs	1
${HOME}/.hadooprc	1
${JAVA}	2
${num}	1
${params}"	1
%-5p	6
%5p	2
%HADOOP_CLIENT_OPTS%	1
%HADOOP_DATANODE_OPTS%	1
%HADOOP_HOME%/logs	1
%HADOOP_HOME%\contrib\capacity-scheduler	1
%HADOOP_JAVA_PLATFORM_OPTS%"	1
%HADOOP_NAMENODE_OPTS%	1
%HADOOP_SECONDARYNAMENODE_OPTS%	1
%USERNAME%	1
%X{op}	2
%YARN_HEAPSIZE%	1
%c:	6
%c{1}	2
%c{2}	5
%c{2}:	5
%m%n	24
%p	11
&	1
'	5
'${httpfs.home}/logs'	1
'${kms.home}/logs'	1
'(i.e.	2
'*'	1
'*',	2
'-'	1
'.	1
'.'	1
':'	1
'\n'	1
'configuration'	1
'dfs.namenode.ec.policies.max.cellsize'	1
'hadoop	1
'httpfs.log.dir'	1
'kms.log.dir'	1
'layoutversion'	1
'mapred	1
'policies'	1
'queue'	1
'queues'	1
'schemas'	1
'xor-2-1'.	1
'xor-2-1-256k',	1
'yarn	1
(	14
(${JAVA_HOME})	1
(%F:%M(%L))	3
(10	2
(256MB)	1
(ASF)	15
(BUT	1
(Hadoop	1
(Java	2
(Note	1
(command)_(subcommand)_USER.	1
(e.g.,	1
(file/dir	1
(fs,	1
(i.e.,	2
(in	1
(period)	1
(primarily)	1
(resource	2
(root	1
(such	1
(superficially)	1
(the	27
)	15
**MUST	1
**MUST**	1
*.period=10	1
*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink	1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30	1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31	1
*.sink.ganglia.period=10	1
*.sink.ganglia.supportsparse=true	1
*NOT*	1
+'%Y%m%d%H%M')"	4
-	11
--	1
-->	40
--config)	1
--daemon	1
-1	1
-1,	1
-9	1
-Dcom.sun.management.jmxremote.authenticate=false	2
-Dcom.sun.management.jmxremote.port=1026"	2
-Dcom.sun.management.jmxremote.ssl=false	2
-Ddatanode.webhdfs.logger=INFO,HTTPDRFA	1
-Dhadoop.home.dir=%HADOOP_YARN_HOME%	1
-Dhadoop.log.dir=%YARN_LOG_DIR%	1
-Dhadoop.log.file=%YARN_LOGFILE%	1
-Dhadoop.root.logger=%YARN_ROOT_LOGGER%	1
-Dhadoop.security.logger=foo).	1
-Dhdfs.audit.logger=%HDFS_AUDIT_LOGGER%	2
-Djava.library.path=%JAVA_LIBRARY_PATH%	1
-Djava.net.preferIPv4Stack=true	1
-Dsun.security.krb5.debug=true	1
-Dsun.security.spnego.debug"	1
-Dyarn.home.dir=%HADOOP_YARN_HOME%	1
-Dyarn.id.str=%YARN_IDENT_STRING%	1
-Dyarn.log.dir=%YARN_LOG_DIR%	1
-Dyarn.log.file=%YARN_LOGFILE%	1
-Dyarn.policy.file=%YARN_POLICYFILE%	1
-Dyarn.root.logger=%YARN_ROOT_LOGGER%	1
-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY"	1
-I	2
-P	1
-P"${HADOOP_SSH_PARALLEL}"	1
-XX:+PrintGCDateStamps	2
-XX:+PrintGCDateStamps"	1
-XX:+PrintGCDetails	3
-XX:+PrintGCTimeStamps	3
-Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date	4
-Xms).	1
-Xmx).	1
-blah).	1
-c	1
-d	1
-f	4
-f)	1
-gt	1
-ls	1
-n	1
-o	2
-s)}	1
-x	1
-z	2
.	1
..	1
...	2
.hadooprc	1
.out	1
/etc/profile.d	2
/tmp	2
0.0	1
1	5
1-MAX_INT.	1
1.	1
1.0.	1
10	1
1000.	1
10000	2
1024(1k)	1
1>&2	3
2.0	27
20	1
246.	1
2NN	1
3.0	1
3.1	1
40	2
40+20=60	1
5gb.	1
64MB	2
8601	1
:	3
;;	1
<!--	40
<-----	1
</acl-administer-jobs>	1
</acl-submit-job>	1
</body>	1
</configuration>	12
</description>	49
</html>	1
</name>	2
</options>	3
</policies>	1
</policy>	2
</properties>	2
</property>	74
</queue>	3
</queues>	1
</schema>	3
</schemas>	1
</table>	1
</tr>	2
</xsl:for-each>	1
</xsl:stylesheet>	1
</xsl:template>	1
<?xml	13
<?xml-stylesheet	6
<LEVEL>,RMSUMMARY	1
<acl-administer-jobs>	1
<acl-submit-job>	1
<body>	1
<cellsize>131072</cellsize>	1
<cellsize>262144</cellsize>	1
<codec>RS-legacy</codec>	1
<codec>RS</codec>	1
<codec>xor</codec>	1
<configuration>	12
<description>	31
<description>ACL	25
<description>Default	1
<description>Keystore	2
<description>Must	2
<description>Optional.	9
<description>Truststore	4
<html>	1
<k>12</k>	2
<k>2</k>	1
<layoutversion>1</layoutversion>	1
<m>1</m>	1
<m>4</m>	2
<name>default.key.acl.DECRYPT_EEK</name>	1
<name>default.key.acl.GENERATE_EEK</name>	1
<name>default.key.acl.MANAGEMENT</name>	1
<name>default.key.acl.READ</name>	1
<name>default</name>	1
<name>hadoop.kms.acl.CREATE</name>	1
<name>hadoop.kms.acl.DECRYPT_EEK</name>	1
<name>hadoop.kms.acl.DELETE</name>	1
<name>hadoop.kms.acl.GENERATE_EEK</name>	1
<name>hadoop.kms.acl.GET</name>	1
<name>hadoop.kms.acl.GET_KEYS</name>	1
<name>hadoop.kms.acl.GET_METADATA</name>	1
<name>hadoop.kms.acl.ROLLOVER</name>	1
<name>hadoop.kms.acl.SET_KEY_MATERIAL</name>	1
<name>q1</name>	1
<name>q2</name>	1
<name>security.admin.operations.protocol.acl</name>	1
<name>security.applicationclient.protocol.acl</name>	1
<name>security.applicationhistory.protocol.acl</name>	1
<name>security.applicationmaster-nodemanager.applicationmaster.protocol.acl</name>	1
<name>security.applicationmaster.protocol.acl</name>	1
<name>security.client.datanode.protocol.acl</name>	1
<name>security.client.protocol.acl</name>	1
<name>security.collector-nodemanager.protocol.acl</name>	1
<name>security.containermanagement.protocol.acl</name>	1
<name>security.datanode.protocol.acl</name>	1
<name>security.distributedscheduling.protocol.acl</name>	1
<name>security.ha.service.protocol.acl</name>	1
<name>security.inter.datanode.protocol.acl</name>	1
<name>security.interqjournal.service.protocol.acl</name>	1
<name>security.job.client.protocol.acl</name>	1
<name>security.job.task.protocol.acl</name>	1
<name>security.mrhs.client.protocol.acl</name>	1
<name>security.namenode.protocol.acl</name>	1
<name>security.qjournal.service.protocol.acl</name>	1
<name>security.refresh.policy.protocol.acl</name>	1
<name>security.refresh.user.mappings.protocol.acl</name>	1
<name>security.resourcelocalizer.protocol.acl</name>	1
<name>security.resourcemanager-administration.protocol.acl</name>	1
<name>security.resourcetracker.protocol.acl</name>	1
<name>security.zkfc.protocol.acl</name>	1
<name>ssl.client.keystore.keypassword</name>	1
<name>ssl.client.keystore.location</name>	1
<name>ssl.client.keystore.password</name>	1
<name>ssl.client.keystore.type</name>	1
<name>ssl.client.truststore.location</name>	1
<name>ssl.client.truststore.password</name>	1
<name>ssl.client.truststore.reload.interval</name>	1
<name>ssl.client.truststore.type</name>	1
<name>ssl.server.exclude.cipher.list</name>	1
<name>ssl.server.keystore.keypassword</name>	1
<name>ssl.server.keystore.location</name>	1
<name>ssl.server.keystore.password</name>	1
<name>ssl.server.keystore.type</name>	1
<name>ssl.server.truststore.location</name>	1
<name>ssl.server.truststore.password</name>	1
<name>ssl.server.truststore.reload.interval</name>	1
<name>ssl.server.truststore.type</name>	1
<name>yarn.scheduler.capacity.application.fail-fast</name>	1
<name>yarn.scheduler.capacity.maximum-am-resource-percent</name>	1
<name>yarn.scheduler.capacity.maximum-applications</name>	1
<name>yarn.scheduler.capacity.node-locality-delay</name>	1
<name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>	1
<name>yarn.scheduler.capacity.queue-mappings-override.enable</name>	1
<name>yarn.scheduler.capacity.queue-mappings</name>	1
<name>yarn.scheduler.capacity.rack-locality-additional-delay</name>	1
<name>yarn.scheduler.capacity.resource-calculator</name>	1
<name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>	1
<name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>	1
<name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>	1
<name>yarn.scheduler.capacity.root.default.capacity</name>	1
<name>yarn.scheduler.capacity.root.default.default-application-lifetime	1
<name>yarn.scheduler.capacity.root.default.maximum-application-lifetime	1
<name>yarn.scheduler.capacity.root.default.maximum-capacity</name>	1
<name>yarn.scheduler.capacity.root.default.state</name>	1
<name>yarn.scheduler.capacity.root.default.user-limit-factor</name>	1
<name>yarn.scheduler.capacity.root.queues</name>	1
<options>	3
<policies>	1
<policy>	2
<properties>	2
<property	2
<property>	74
<queue>	3
<queues>	1
<schema	3
<schema>RS-legacyk12m4</schema>	1
<schema>XORk2m1</schema>	1
<schemas>	1
<state>running</state>	1
<table	1
<td><a	1
<td><xsl:value-of	2
<td>description</td>	1
<td>name</td>	1
<td>value</td>	1
<tr>	2
<value>*</value>	41
<value>-1</value>	3
<value>0.1</value>	1
<value>10000</value>	3
<value>100</value>	2
<value>1</value>	2
<value>40</value>	1
<value></value>	11
<value>RUNNING</value>	1
<value>TLS_ECDHE_RSA_WITH_RC4_128_SHA,SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA,	1
<value>default</value>	1
<value>false</value>	2
<value>jks</value>	4
<value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>	1
<xsl:for-each	1
<xsl:output	1
<xsl:stylesheet	1
<xsl:template	1
=	1
>	12
@echo	3
@rem	80
A	26
ACL	40
ACL,	2
ACLs	4
ACTS	3
ALL	5
ALL!)	1
AN	2
AND	3
ANY	27
AS	3
ASF	15
AWS	1
Add	1
AdminOperationsProtocol.	1
Advanced	2
All	4
An	1
Any	2
Apache	44
App	1
AppSummaryLogging	1
Appender	8
Apple	1
Application	3
ApplicationClientProtocol,	1
ApplicationHistoryProtocol,	1
ApplicationMaster	1
ApplicationMasterProtocol,	2
ApplicationMasters	3
Automatically	1
BASIS,	27
BE	1
BY	3
Bail	1
Balancer	1
Balancer.	1
BatchMode=yes	1
Below	1
BlockManager	1
But	2
By	7
CAN	2
CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE	1
CLASSPATH	2
CLASSPATH.	1
CLASSPATH?	1
COMMANDS.	3
CONDITIONS	27
CONFIGURATION	1
CPU	1
CREATE	1
Cache	1
CapacityScheduler	2
Change	1
ClientDatanodeProtocol,	1
ClientProtocol,	1
CollectorNodemanagerProtocol,	1
Comma	1
Command	1
Commons	1
Complementary	1
Configuration	1
Configuring	1
ConnectTimeout=10s"	1
Container	1
ContainerManagementProtocol	1
Controller	1
Controls	1
Counter	1
CryptoExtension	2
Currently,	1
Custom	1
DECRYPT_EEK	1
DN.	2
DNS	2
DNS,	2
DONE	2
Daemons	2
Daily	2
Darwin*)	1
DataNode	2
DataNode.	1
DatanodeProtocol,	1
Date	2
Debugging	2
Default	21
DefaultResourceCalculator	1
Define	2
Directory	1
DistributedFileSystem.	1
DistributedSchedulingAMProtocol,	1
Docker	1
DominantResourceCalculator	1
During	1
EC	5
Empty	1
Enable	2
Enable/Disable	1
Entries	1
Eons	1
Event	1
EventCounter	1
Example:	3
Examples	1
Extra	4
FILE	5
FOR	5
FPGA	3
FPGA,	1
Failover	1
Fair	1
Federation	2
File	2
FileSystem	1
Filename	1
For	35
Foundation	15
GENERATE_EEK	1
GET	2
Ganglia	3
Gateway	1
Gateway.	1
Generic	1
HAAdmin	1
HADOOP	3
HADOOP_CLASSPATH	3
HADOOP_CLASSPATH="/some/cool/path/on/your/machine"	1
HADOOP_CLASSPATH=%HADOOP_CLASSPATH%;%HADOOP_HOME%\contrib\capacity-scheduler\*.jar	1
HADOOP_CLASSPATH=%HADOOP_HOME%\contrib\capacity-scheduler\*.jar	1
HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES	1
HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES="-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop."	1
HADOOP_CLIENT_OPTS=""	1
HADOOP_CLIENT_OPTS=-Xmx512m	1
HADOOP_CONF_DIR=	1
HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop	1
HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA	1
HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS	1
HADOOP_DFSROUTER_OPTS=""	1
HADOOP_ENABLE_BUILD_PATHS="true"	1
HADOOP_GC_SETTINGS="-verbose:gc	1
HADOOP_HEAPSIZE=	1
HADOOP_HEAPSIZE_MAX	1
HADOOP_HEAPSIZE_MAX.	4
HADOOP_HEAPSIZE_MAX=	1
HADOOP_HEAPSIZE_MIN=	1
HADOOP_HOME=	1
HADOOP_IDENT_STRING	3
HADOOP_IDENT_STRING=$USER	1
HADOOP_IDENT_STRING=%USERNAME%	1
HADOOP_JAAS_DEBUG=true	1
HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData	1
HADOOP_JHS_LOGGER=INFO,RFA	1
HADOOP_JOB_HISTORYSERVER_HEAPSIZE=	1
HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000	1
HADOOP_LOG_DIR=${HADOOP_HOME}/logs	1
HADOOP_LOG_DIR=%HADOOP_LOG_DIR%\%USERNAME%	1
HADOOP_MAPRED_ROOT_LOGGER=%HADOOP_LOGLEVEL%,RFA	1
HADOOP_NAMENODE_INIT_HEAPSIZE=""	1
HADOOP_NAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER%	1
HADOOP_NICENESS=0	1
HADOOP_OPTIONAL_TOOLS="hadoop-aliyun,hadoop-openstack,hadoop-azure,hadoop-azure-datalake,hadoop-aws,hadoop-kafka"	1
HADOOP_OPTS	46
HADOOP_OPTS,	1
HADOOP_OPTS="-Djava.net.preferIPv4Stack=true	1
HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"	1
HADOOP_OPTS=%HADOOP_OPTS%	1
HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname	1
HADOOP_PID_DIR=%HADOOP_PID_DIR%	1
HADOOP_PID_DIR=/tmp	1
HADOOP_POLICYFILE="hadoop-policy.xml"	1
HADOOP_ROOT_LOGGER=INFO,console	1
HADOOP_SECONDARYNAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER%	1
HADOOP_SECURE_DN_LOG_DIR=%HADOOP_LOG_DIR%\%HADOOP_HDFS_USER%	1
HADOOP_SECURE_DN_PID_DIR=%HADOOP_PID_DIR%	1
HADOOP_SECURE_DN_USER=%HADOOP_SECURE_DN_USER%	1
HADOOP_SECURE_IDENT_PRESERVE="true"	1
HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}	1
HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}	1
HADOOP_SECURITY_LOGGER	1
HADOOP_SECURITY_LOGGER=INFO,NullAppender	1
HADOOP_SECURITY_LOGGER=INFO,RFAS	1
HADOOP_SSH_OPTS	1
HADOOP_SSH_OPTS="-o	1
HADOOP_SSH_PARALLEL=10	1
HADOOP_STOP_TIMEOUT=5	1
HADOOP_USER_CLASSPATH_FIRST	1
HADOOP_USER_CLASSPATH_FIRST="yes"	1
HADOOP_USE_CLIENT_CLASSLOADER	2
HADOOP_USE_CLIENT_CLASSLOADER=true	1
HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"	1
HADOOP_YARN_USER	1
HADOOP_YARN_USER=%yarn%	1
HADOOP_xx_SECURE_USER.	1
HADOOP_xyz	3
HAService	1
HDFS	13
HDFS,	1
HDFS_AUDIT_LOGGER	1
HDFS_AUDIT_LOGGER=INFO,NullAppender	2
HDFS_BALANCER_OPTS=""	1
HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS"	1
HDFS_DATANODE_SECURE_EXTRA_OPTS="-jvm	1
HDFS_DATANODE_SECURE_USER=hdfs	1
HDFS_DFSROUTER_OPTS=""	1
HDFS_JOURNALNODE_OPTS=""	1
HDFS_MOVER_OPTS=""	1
HDFS_NAMENODE_OPTS="${HADOOP_GC_SETTINGS}	1
HDFS_NAMENODE_OPTS="-Dcom.sun.management.jmxremote=true	1
HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"	1
HDFS_NAMENODE_OPTS="-verbose:gc	1
HDFS_NAMENODE_USER=hdfs	1
HDFS_NFS3_OPTS=""	1
HDFS_NFS3_SECURE_EXTRA_OPTS="-jvm	1
HDFS_NFS3_SECURE_USER=nfsserver	1
HDFS_PORTMAP_OPTS="-Xmx512m"	1
HDFS_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS"	1
HDFS_STORAGECONTAINERMANAGER_OPTS=""	1
HDFS_ZKFC_OPTS=""	1
HERE	1
HS	1
HSClientProtocol,	1
HTTP	6
HTTPFS	4
HTTPFS_CONFIG=${HADOOP_CONF_DIR}	1
HTTPFS_HTTP_HOSTNAME=$(hostname	1
HTTPFS_HTTP_PORT=14000	1
HTTPFS_LOG=${HADOOP_LOG_DIR}	1
HTTPFS_MAX_HTTP_HEADER_SIZE=65536	1
HTTPFS_MAX_THREADS=1000	1
HTTPFS_SSL_ENABLED=false	1
HTTPFS_SSL_KEYSTORE_FILE=${HOME}/.keystore	1
HTTPFS_SSL_KEYSTORE_PASS=password	1
HTTPFS_TEMP=${HADOOP_HDFS_HOME}/temp	1
Hadoop	14
Hadoop's	2
Hadoop,	2
Hadoop-specific	2
Hadoop.	1
Here	1
History	2
HistoryServer.	1
How	1
However,	1
However...	1
Http	1
HttpFS	1
HttpFSServer	1
ID	2
ID,	1
INFO,NullAppender	1
INSTEAD	1
IPv4.	1
IPv6	1
IS"	27
ISO	1
If	31
In	3
Increasing	1
InterDatanodeProtocol,	1
InterQJournalProtocol,	1
Irrespective	2
It	9
JAAS	1
JAVA="${JAVA_HOME}/bin/java"	1
JAVA_HEAP_MAX=-Xmx%YARN_HEAPSIZE%m	1
JAVA_HOME	3
JAVA_HOME.	2
JAVA_HOME=	1
JAVA_HOME=$(/usr/libexec/java_home)	1
JAVA_HOME=%JAVA_HOME%	1
JAVA_HOME=/usr/java/testing	1
JAVA_LIBRARY_PATH	1
JDK:	1
JMX	2
JN	2
JNs	1
JSVC_HOME=%JSVC_HOME%	1
JSVC_HOME=/usr/bin	1
JVM	22
Java	24
Java's	1
Job	2
JobHistoryServer	1
JobHistoryServer.	1
Jsvc	3
KIND,	27
KMS	6
KMS_CONFIG=${HADOOP_CONF_DIR}	1
KMS_HTTP_PORT=9600	1
KMS_LOG=${HADOOP_LOG_DIR}	1
KMS_MAX_HTTP_HEADER_SIZE=65536	1
KMS_MAX_THREADS=1000	1
KMS_SSL_ENABLED=false	1
KMS_SSL_KEYSTORE_FILE=${HOME}/.keystore	1
KMS_SSL_KEYSTORE_PASS=password	1
KMS_TEMP=${HADOOP_HOME}/temp	1
Kerberos	2
LICENSE	9
Legal	1
License	81
License,	27
License.	54
Licensed	27
Location	2
Log	5
LogLevel	2
LogMessage	2
LoggerName	2
Logging	2
Logs	1
Lower	1
MANAGEMENT	1
MAPREDUCE	1
MAPRED_HISTORYSERVER_OPTS.	1
MAPRED_HISTORYSERVER_OPTS=	1
MAPRED_xyz	1
MASTER	1
MB.	8
MR	2
MRClientProtocol,	1
Major	1
Manager	3
Manager.	1
Many	3
Map/Reduce	2
Master	1
Maximum	3
Memory	1
Memory,	1
Metrics.	1
Modifications	1
Most	1
Mover	1
Mover.	1
Must	4
NFS3	2
NN	4
NNs,	1
NOT	2
NOT**	1
NOTE:	3
NOTICE	15
Name	1
NameNode	2
NameNode.	2
NamenodeProtocol,	1
Names	1
Node	1
NodeManager	3
NodeManager.	2
Nodemanager	2
Note	6
Note,	1
Null	1
Number	2
OF	28
OFF_SWITCH	2
ONE	1
OPTIONS	1
OR	27
OS	1
OVERRIDE	2
OVERWRITING	1
On	3
Only	1
Only!	1
Options	3
Otherwise	1
Out	1
PROJECTS.	1
Pattern	4
Precedence	3
Print	1
Protocols	1
Proxy	1
Put	5
QJournalProtocol,	1
Queue	1
QuorumJournalManager	1
QuorumJournalNode	1
QuorumJournalNode.	1
RBF	2
READ	2
RELATED	2
REQUIRED	1
RM	1
RM,	1
ROLLOVER	1
RUNNING	1
RefreshAuthorizationPolicyProtocol,	1
RefreshUserMappingsProtocol.	1
Registry	1
Request	1
Required.	1
Requires	1
Resource	1
ResourceCalculator	1
ResourceLocalizer	2
ResourceManager	13
ResourceManager.	2
ResourceManagerAdministrationProtocol,	1
ResourceTrackerProtocol,	1
Resourcemanager	1
Resources	1
Rolling	3
Rollover	1
Router	1
Router-based	2
Router.	1
Routers.	2
S3A	1
SASL	2
SDK	1
SET	1
SETTINGS	1
SSH	1
SSL	11
SSL_RSA_EXPORT_WITH_RC4_40_MD5,SSL_RSA_EXPORT_WITH_DES40_CBC_SHA,	1
SSL_RSA_WITH_DES_CBC_SHA,SSL_DHE_RSA_WITH_DES_CBC_SHA,	1
SSL_RSA_WITH_RC4_128_MD5</value>	1
STOPPED.	1
SecondaryNameNode	1
SecondaryNameNode.	1
Secure/privileged	1
Security	1
See	60
Sends	1
Server	4
Services	1
Set	11
Setup	2
Shared	1
Should	1
Similarly,	1
Site	1
Site-wide	1
Slider	1
So	2
Software	15
Some	2
Specifies	2
Specify	27
Specifying	1
State	2
Storage	1
StorageContainerManager	1
StrictHostKeyChecking=no	1
Strongly	1
Summary	3
Sun/Oracle	1
Supplemental	3
System	2
THE	3
THEREFORE,	1
THIS	4
TO	1
Tag	1
TaskLog	1
TaskUmbilicalProtocol,	1
Technically,	1
The	108
There	3
Therefore,	3
These	21
This	27
Threshold	1
TimeLineReader	1
TimeLineReader.	1
TimeLineServer	1
TimeLineServer.	1
To	3
Typically	1
USE	1
Uncomment	3
Unless	27
Use	2
Used	2
User	2
Users	2
Version	27
WARRANTIES	27
WILL	1
WITHOUT	27
WORK	2
We	4
Web	1
WebHdfs	1
When	5
Where	5
Whether	3
With	1
X!	1
XML	4
Xms	1
Xmx	6
YARN	5
YARN,	1
YARN.	1
YARN_CONF_DIR	1
YARN_CONF_DIR=%HADOOP_YARN_HOME%\conf	1
YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE=true	1
YARN_HEAPSIZE	1
YARN_LOGFILE	1
YARN_LOGFILE=yarn.log	1
YARN_LOG_DIR	1
YARN_LOG_DIR=%HADOOP_YARN_HOME%\logs	1
YARN_NODEMANAGER_HEAPSIZE=	1
YARN_NODEMANAGER_OPTS.	1
YARN_NODEMANAGER_OPTS=	1
YARN_OPTS=%YARN_OPTS%	11
YARN_POLICYFILE	1
YARN_POLICYFILE=hadoop-policy.xml	1
YARN_PROXYSERVER_HEAPSIZE=	1
YARN_PROXYSERVER_OPTS.	1
YARN_PROXYSERVER_OPTS=	1
YARN_REGISTRYDNS_SECURE_EXTRA_OPTS="-jvm	1
YARN_REGISTRYDNS_SECURE_USER=yarn	1
YARN_RESOURCEMANAGER_HEAPSIZE=	1
YARN_RESOURCEMANAGER_OPTS.	1
YARN_RESOURCEMANAGER_OPTS=	1
YARN_RESOURCEMANAGER_OPTS="-Dcom.sun.management.jmxremote=true	1
YARN_RESOURCEMANAGER_OPTS="-Dyarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log	1
YARN_RESOURCEMANAGER_OPTS="-verbose:gc	1
YARN_RESOURCE_MANAGER_OPTS="${HADOOP_GC_SETTINGS}	1
YARN_ROOT_LOGGER	1
YARN_ROOT_LOGGER=%HADOOP_LOGLEVEL%,console	1
YARN_ROUTER_OPTS=	1
YARN_SERVICE_EXAMPLES_DIR	1
YARN_SHAREDCACHEMANAGER_OPTS=	1
YARN_TIMELINEREADER_OPTS=	1
YARN_TIMELINESERVER_OPTS.	1
YARN_TIMELINESERVER_OPTS=	1
YARN_TIMELINE_HEAPSIZE=	1
YARN_xyz	1
Yarn	1
You	39
ZK	1
ZKFailoverController	1
ZKFailoverController.	1
[%X{hostname}][%X{user}:%X{doAs}]	2
[%t]	3
[[	7
[prefix].[source|sink].[instance].[options]	1
[user={name}	1
[u|g]:[name]:[queue_name][,next	1
\	2
]];	7
_OPT	2
_OPTS	2
a	116
a)	2
about	1
above	3
accept	2
access	1
accidentally	1
accidents,	1
accompanying	9
accordingly.	1
account.	2
acl	1
acls	5
active	1
add	3
added	2
added/removed	1
additional	16
additional,	1
additions	1
admin	2
administer	1
administrators	3
after	11
ago,	1
agreed	27
agreements.	15
aka	1
all	41
allow	3
allowed	5
allowed.</description>	21
allowed.system.users=##comma	1
allows	2
almost	1
along	1
already	2
also	5
always	1
amlog	1
amount	3
an	39
and	150
and/or	6
any	25
app	3
append	1
append.	1
appended	22
appender	3
appender)	1
applicable	29
applicable,	1
application	8
applications	7
applications'	1
applications.	1
applies	1
appropriate	2
approximately	1
apps/end-users	1
appsummary	1
are	59
argument	1
arguments	1
as	57
assign	1
assigning	1
assignments	3
assumed	5
assumes	1
at	34
attack.	1
attempt	4
attempts	2
audit	2
authentication	4
authorization	5
autoscale	2
avoiding	1
awesome-methods-1.0.jar,	2
b	1
b)	2
back	1
backup	1
banned.users=#comma	1
based	6
bash	3
basis	2
be	115
being	1
below	2
best	1
bind	1
binding,	1
bit	1
blank	1
blank.	21
block	4
block:	1
bogus.	1
border="1">	1
both	2
box,	1
bridge,host,none	1
bring	1
building	1
built	1
built-in	1
bundled	1
but	1
by	113
c)	2
cache	1
calculated	1
called	2
can	29
can't	2
cannot	1
cap	1
capabilities	1
capacity	1
capacity-scheduler.	1
capacity.</description>	1
case	4
case,	1
cases,	2
cellsize	2
cellsize(in	1
cellsize.	1
certain	3
certainly	1
change	4
changes	1
changing	3
check	4
checked	1
checks	1
child	1
cipher	1
class	1
classes	1
classloader	2
classpath	3
classpaths.	1
client	4
client-to-datanode	1
clients	6
cluster	7
cluster.	1
code	5
codec	1
codec,	1
collection	3
collector	1
combination	2
comma	8
comma-separated	21
command	5
command,	1
command.	1
commands	6
commands.	4
commas.	1
common	1
common,	1
communciate	2
communicate	12
communicate.	2
communication.</description>	1
compare	2
compliance	27
components	1
compression	1
computed	1
concurrent	1
config	3
config,	2
configs	4
configuration	10
configuration,	2
configuration.	6
configuration:	1
configure	2
configured	11
connection	1
connections	1
connections.	1
connectivity	1
consider	1
considered	2
consists	1
console	1
constraint	1
contain	3
container	2
containers	2
containers,	1
containers.	2
containing	1
contains	5
context	3
context.	1
contexts	1
contributor	15
control	2
controls	1
conversion	1
converted	2
copy	27
copyright	15
correctly	1
could	2
counts	1
create-key	1
creating	1
creation	1
custom	2
d)	1
daemon	3
daemon,	1
daemons	5
daemons"	1
daemons,	1
daemons.	2
data	5
datanode	3
datanode-metrics.log	1
datanode.metrics.logger=INFO,NullAppender	1
datanodes	4
datanodes,	2
datanodes.	1
date	1
de-deduplication).	1
de-deduplication,	1
deal	3
debug	2
debugging	2
debugging,	1
decryptEncryptedKey	1
default	31
default!	1
default,	8
default.	7
default.)	1
default:	3
default;	2
default_priority={priority}]	1
defaults	9
define	1
define:	1
defined	23
defined,	1
defined.	4
defines	2
defining	1
definition	1
delay	1
delete-key	1
delimited	1
dependent	1
deprecation	1
desirable,	1
details	1
detect	1
determine	1
device	2
devices	2
dfs	2
dfs,	1
dfs.namenode.rpc-address	1
dfs.namenode.servicerpc-address	1
dfsadmin	1
did	1
different	6
differently	1
direct	1
directly	2
directories	2
directory	10
directory)	1
dirs	1
disable=SC2086	1
disabled	3
disabled.	2
disables	2
distcp	1
distcp.	2
distributed	70
do	9
docker	1
docker.allowed.capabilities=##	1
docker.allowed.devices=##	1
docker.allowed.networks=##	1
docker.allowed.ro-mounts=##	1
docker.allowed.runtimes=##	1
docker.allowed.rw-mounts=##	1
docker.allowed.volume-drivers=##	1
docker.binary=/usr/bin/docker	1
docker.no-new-privileges.enabled=##	1
docker.privileged-containers.enabled=false	1
document	1
document.	2
does	1
dominant-resource	1
don't	2
done	1
dropping	4
dump	2
during	2
e.g	2
e.g,	1
e.g.	21
each	10
each.	2
easier	1
echo	5
edit	1
edition	1
efficient	1
either	31
element	2
element.	1
elements.	1
else	1
empty	2
enable	3
enable,	3
enable/disable	2
enabled	4
enabled,	1
enabled.	1
encoding="UTF-8"?>	5
end	1
ending	1
ends	1
entries	1
env	1
environment	8
environment.	1
equal	2
equivalent.	1
errors	1
esac	1
etc)	1
etc.	5
example	3
example,	7
example:	2
examples	7
exceed	1
exceeds	2
except	28
excluded	1
executable."	1
execute	2
executed	1
execution	5
exist	2
exist."	1
exists,	1
exit	3
expanded	1
expensive	1
explicit	2
explicitly	5
export	83
express	27
extended	1
extra	1
fail	1
false	1
false.	1
fanout	1
feature	3
feature.	1
feature.tc.enabled=false	1
features	1
fi	6
file	71
file,	1
file.	17
file:	1
filename	1
filename)	1
files	9
files)	1
files,	1
find	1
finding	1
first	2
flag	1
flags	20
following	11
for	178
force	1
format	7
format,	6
format:	2
found."	1
fpga.allowed-device-minor-numbers=##	1
fpga.major-device-number=##	1
framework	1
from	11
fsck,	1
function	4
functionality	1
functionality,	1
functions.	1
ganglia	1
garbage	3
gateway	1
gateways	1
gateways,	1
gathered:	2
generate,	1
generateEncryptedKey	1
generation	1
generic	1
get	1
get-current-key	1
get-key-metadata	1
get-key-version	1
get-keys	1
get-keys-metadata	1
gets	1
given,	5
governing	27
granted,	1
greater	1
group	42
group1,group2	2
group={name}	1
groups	2
gzip	1
hadoop	4
hadoop,	1
hadoop-env.sh	9
hadoop-functions.sh	1
hadoop-functions.sh.	1
hadoop.	2
hadoop.id.str	4
hadoop.log.dir	3
hadoop.log.dir=.	1
hadoop.log.file=hadoop.log	1
hadoop.log.maxbackupindex=20	1
hadoop.log.maxfilesize=256MB	1
hadoop.mapreduce.jobsummary.log.file	1
hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log	1
hadoop.mapreduce.jobsummary.log.maxbackupindex=20	1
hadoop.mapreduce.jobsummary.log.maxfilesize=256MB	1
hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}	1
hadoop.mapreduce.jobsummary.logger=INFO,JSA	1
hadoop.policy.file	1
hadoop.root.logger	3
hadoop.root.logger=INFO,console	1
hadoop.security.log.file=SecurityAuth-${user.name}.audit	1
hadoop.security.log.maxbackupindex=20	1
hadoop.security.log.maxfilesize=256MB	1
hadoop.security.logger	1
hadoop.security.logger=INFO,NullAppender	1
hadoop_actual_ssh	1
hadoop_add_classpath	1
hadoop_connect_to_hosts_without_pdsh	1
hadoop_java_setup	1
hadoop_rotate_log	2
handled	2
handler	4
hard	1
hard-coded	6
hard-set	1
harder	1
has	3
have	2
hdfs	3
hdfs,	1
hdfs.audit.log.maxbackupindex=20	1
hdfs.audit.log.maxfilesize=256MB	1
hdfs.audit.logger	1
hdfs.audit.logger=INFO,NullAppender	1
header	2
heap	3
heapsize	6
heartbeat.	1
helper	2
here	6
here.	5
hierarchical	2
his/her	1
history	1
host	1
hostname	3
hosts	1
hosts.	1
hot-reloaded	1
href="configuration.xsl"?>	6
http://www.apache.org/licenses/LICENSE-2.0	27
httpfs	4
httpfsaudit	1
i.e.	2
i.e.,	1
id	3
id="RS-legacyk12m4">	1
id="RSk12m4">	1
id="XORk2m1">	1
ideally	1
identified.	1
if	38
ignored,	1
ignored.	1
implementation	5
implied.	27
improve	2
in	137
in-effect.	1
include	1
included	1
increase/decrease	1
information	17
information.	2
insensitive	2
insert	1
instance	2
instead	3
integer	2
inter-datanode	1
interactive	2
internally	1
interval,	2
into	6
irrespective	1
is	186
isolated	1
it	29
it's	1
it.	2
its	6
itself	1
jar	1
jar',	2
java	3
java_home	1
javadoc	1
jetty	1
job	6
job'	1
jobs	11
jobs,	1
jobs.	2
jsvc	6
just	1
jvm.	3
k)	1
k,	1
keep	3
key	7
key.	1
key="capacity"	1
key="user-limit"	1
keystore	8
killed	1
killing	2
kms	2
kms-audit	1
know	3
language	27
last	1
later	1
launch	4
law	27
layout	1
leaf	4
left	1
less	2
let	2
level	13
level!	1
levels	3
libraries	1
license	15
licenses	15
lifetime	6
lifetime.	3
like	3
limit	4
limitations	27
line	4
line.	4
lines	1
list	53
list.	1
listed	1
living.	1
local	4
localhost	1
locality	1
locate	1
location	5
locations	1
locked	1
log	23
log4j	4
log4j.additivity.DataNodeMetricsLog=false	1
log4j.additivity.NameNodeMetricsLog=false	1
log4j.additivity.kms-audit=false	1
log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false	1
log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false	1
log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false	1
log4j.appender.DNMETRICSRFA.File=${hadoop.log.dir}/datanode-metrics.log	1
log4j.appender.DNMETRICSRFA.MaxBackupIndex=1	1
log4j.appender.DNMETRICSRFA.MaxFileSize=64MB	1
log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd	1
log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}	1
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd	1
log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}	1
log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}	1
log4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}	1
log4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}	1
log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender	1
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter	1
log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}	1
log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}	1
log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}	1
log4j.appender.JSA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.JSA=org.apache.log4j.RollingFileAppender	1
log4j.appender.NNMETRICSRFA.File=${hadoop.log.dir}/namenode-metrics.log	1
log4j.appender.NNMETRICSRFA.MaxBackupIndex=1	1
log4j.appender.NNMETRICSRFA.MaxFileSize=64MB	1
log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender	1
log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}	1
log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}	1
log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}	1
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log	1
log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}	1
log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}	1
log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender	1
log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}	1
log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}	1
log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}	1
log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFAS=org.apache.log4j.RollingFileAppender	1
log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}	1
log4j.appender.RMSUMMARY.MaxBackupIndex=20	1
log4j.appender.RMSUMMARY.MaxFileSize=256MB	1
log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender	1
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.TLA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender	1
log4j.appender.amlog.File=${LOG_DIR}/serviceam.log	1
log4j.appender.amlog.MaxBackupIndex=20	1
log4j.appender.amlog.MaxFileSize=256MB	1
log4j.appender.amlog.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.amlog.layout=org.apache.log4j.PatternLayout	1
log4j.appender.amlog=org.apache.log4j.RollingFileAppender	1
log4j.appender.console.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.console.layout=org.apache.log4j.PatternLayout	1
log4j.appender.console.target=System.err	1
log4j.appender.console=org.apache.log4j.ConsoleAppender	1
log4j.appender.httpfs.Append=true	1
log4j.appender.httpfs.DatePattern='.'yyyy-MM-dd	1
log4j.appender.httpfs.File=${httpfs.log.dir}/httpfs.log	1
log4j.appender.httpfs.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.httpfs.layout=org.apache.log4j.PatternLayout	1
log4j.appender.httpfs=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.httpfsaudit.Append=true	1
log4j.appender.httpfsaudit.DatePattern='.'yyyy-MM-dd	1
log4j.appender.httpfsaudit.File=${httpfs.log.dir}/httpfs-audit.log	1
log4j.appender.httpfsaudit.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.httpfsaudit.layout=org.apache.log4j.PatternLayout	1
log4j.appender.httpfsaudit=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.kms-audit.Append=true	1
log4j.appender.kms-audit.DatePattern='.'yyyy-MM-dd	1
log4j.appender.kms-audit.File=${kms.log.dir}/kms-audit.log	1
log4j.appender.kms-audit.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.kms-audit.layout=org.apache.log4j.PatternLayout	1
log4j.appender.kms-audit=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.kms.Append=true	1
log4j.appender.kms.DatePattern='.'yyyy-MM-dd	1
log4j.appender.kms.File=${kms.log.dir}/kms.log	1
log4j.appender.kms.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.kms.layout=org.apache.log4j.PatternLayout	1
log4j.appender.kms=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.stderr.Target=System.err	1
log4j.appender.stderr.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.stderr.layout=org.apache.log4j.PatternLayout	1
log4j.appender.stderr=org.apache.log4j.ConsoleAppender	1
log4j.appender.subprocess.layout.ConversionPattern=[%c{1}]:	1
log4j.appender.subprocess.layout=org.apache.log4j.PatternLayout	1
log4j.appender.subprocess=org.apache.log4j.ConsoleAppender	1
log4j.category.SecurityLogger=${hadoop.security.logger}	1
log4j.logger.DataNodeMetricsLog=${datanode.metrics.logger}	1
log4j.logger.NameNodeMetricsLog=${namenode.metrics.logger}	1
log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR	1
log4j.logger.com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator=OFF	1
log4j.logger.httpfsaudit=INFO,	1
log4j.logger.kms-audit=INFO,	1
log4j.logger.org.apache.commons.beanutils=WARN	1
log4j.logger.org.apache.curator.framework.imps=WARN	1
log4j.logger.org.apache.curator.framework.state=ERROR	1
log4j.logger.org.apache.curator=INFO	1
log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN	1
log4j.logger.org.apache.hadoop.fs.http.server=INFO,	1
log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}	1
log4j.logger.org.apache.hadoop.hdfs=WARN	1
log4j.logger.org.apache.hadoop.lib=INFO,	1
log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}	1
log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG	1
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR	1
log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}	1
log4j.logger.org.apache.hadoop=INFO	1
log4j.logger.org.apache.zookeeper=INFO	1
log4j.logger.org.apache.zookeeper=WARN	1
log4j.logger.org.eclipse.jetty=INFO	1
log4j.rootLogger=${hadoop.root.logger},	1
log4j.rootLogger=INFO,	2
log4j.threshhold=ALL	1
log4j.threshold=ALL	1
log=$1;	1
logger	3
logging	4
logging.	2
logs	4
logs.</description>	1
longer	1
loop,	1
low	1
m	1
machine	2
main	1
majority	1
make	2
manage	1
managed	1
manager	4
many	3
map	2
mapping	1
mapping]*	1
mappings	1
mappings.	1
mapred	2
mapred-env.sh	1
mapreduce.cluster.acls.enabled	3
mapreduce.cluster.administrators	2
maps	1
masters	1
match="configuration">	1
matches.	1
material	3
max	6
max_priority={priority}	1
maximum	10
may	80
means	22
memory	2
menthod	1
mentions	1
messages	2
messages.	2
method	1
method="html"/>	1
metric	1
metrics	7
midnight	1
milliseconds.	2
min.user.id=1000#Prevent	1
mind!	1
minimum	1
minor	1
missed	5
mode.	3
modify	2
modifying	1
module.	2
module.enabled=##	2
moral	1
more	18
most	1
mounted	3
mradmin	1
much	2
multi-dimensional	1
multiple	3
must	3
mv	2
name	5
name.	1
name="{name}"><xsl:value-of	1
namenode	2
namenode-metrics.log	1
namenode-metrics.out	1
namenode.	3
namenode.</description>	1
namenode.metrics.logger=INFO,NullAppender	1
namenode:	1
namenodeAddress:9110	1
namenodeAddress:9111	1
names	1
names,	1
names.	21
necessary.	1
needed.	1
needs	3
negative	1
nesting	1
networks	1
new	3
next	1
no	15
no-new-privileges	1
node's	1
node-locality-delay	1
node-locality-delay=40	1
nodemanager	2
nodes	1
nodes.	2
noise	1
non-namenode	1
non-privileged	2
non-secure)	1
normal	1
not	64
note	1
num=${2:-5};	1
num=${prev}	1
number	9
numbers	1
numbers,	1
obtain	27
of	169
off	4
off-switch	3
official	1
on	45
on.	1
one	26
ones,	1
ones.	1
only	14
operating	1
operation.	3
operations	9
operations.	8
opportunities	3
opportunities,	1
opportunities.	1
option	4
optional	1
optional,	1
optional.	2
options	73
options.	1
or	79
org.apache.hadoop.metrics2	1
other	3
other.	7
others	2
otherwise.	1
out	2
output	2
outside	2
over	1
overridden	8
override	23
override.	1
overrides	7
owner	1
ownership.	15
package	1
package-info.java	1
parameter,	2
parameters	21
parent	1
part	2
particular	1
parts	2
pass	1
passing	2
password	2
path.	2
paths	1
pattern	1
pause	1
pdsh	1
pdsh.	1
pending	1
per	2
per-daemon	1
percent	1
percentage	1
performance,	1
period	1
period,	1
permissions	27
perspective	1
pick	1
pid	3
pids	1
pipe.	1
place	2
placeholder	1
platforms	1
point-in-time	2
policies	2
policies,	1
policy	5
policy,	1
port	3
port.	1
portmapper.	1
ports	2
ports.	3
positive	2
potential	1
prefer	2
preferable,	1
preference	1
preferred.	1
prefix.	1
present,	1
prev=${num}-1	1
prevent	1
previous	1
prior	2
priorities.	1
priority	1
priority.	1
privileged	8
privileges	3
privileges.	1
priviliged	1
probably	1
problems	1
process	1
processes.	2
program	1
properties	5
properties.	1
property	19
property:	9
protocol	6
protocol,	2
protocol.	2
provide	4
provided,	2
providing	1
proxy	2
push	1
q1.	1
q2	2
q2.	1
query	1
queue	17
queue).	1
queue,	1
queue.	13
queues	9
queues,	1
queues.	3
quoting	1
rack-local	3
rack-locality-delay=20,	1
rack.	1
rate	1
re-use	1
re-used	1
react	1
read	3
read-only	1
read-write,	1
reading	1
recommend	2
recovery	1
recovery.	1
redefining	1
reduce	2
reference	1
referred	1
refresh	2
refreshable.	1
regarding	15
registry	2
related	1
reload	2
remote	4
replace	6
replaced	1
representing	2
request	2
request,	1
required	32
resource	3
resources	2
respective	2
response.	2
result	1
retain	2
returned	2
right	1
rolling	1
rollover-key	1
root	2
rootLogger.	2
rootlogger	1
rotation	2
rpc	2
rpc.rpc.NumOpenConnections	1
rpc.rpc.port=9110.NumOpenConnections	1
rpc.rpc.port=9111.NumOpenConnections	1
rules:	3
run	13
run.	1
running	3
running,	1
running.	1
runs	1
runtime	3
runtimes	1
same	7
sample	1
sampling	2
schedule	2
scheduler	2
scheduler.	1
schedulers,	1
scheduling	3
scheduling.	1
schema	6
schema,	1
schemas	2
scripts	2
secondary	1
seconds	2
seconds).	2
seconds.	2
secret	1
section	1
secure	14
security	3
security-related	1
security.	1
seen.	1
segment	1
select="description"/></td>	1
select="name"/></a></td>	1
select="property">	1
select="value"/></td>	1
send	1
sending	1
separate	5
separated	24
separating	1
seperate	1
seperated	6
series	1
server	5
server"	3
server.	3
service	5
service-level	1
set	86
set,	2
sets	3
setting	14
settings	5
severity	1
shared	1
shell	4
shellprofile	1
short-cut,	1
should	9
shuffle	2
shuffleHandler	1
similar	20
simple.	1
simultaneous	1
sinks	1
site-specific	5
sites	1
size	7
size.	2
skips	1
so	5
software	27
some	10
something	1
sooner.	1
space	1
space),	2
spawned	1
special	24
specifc	1
specific	52
specified	31
specified.	6
specify	1
specifying	3
split	1
ssh	1
stack-trace	1
stand-by	1
standardized	1
start	2
start-dfs.sh	1
start-dfs.sh,	1
start/stop	1
started	2
starting	21
starts	1
startup	1
state	6
states	1
status	2
stderr	2
stdin/pipe.	2
stopped,	1
stopping	1
stored	1
stored.	4
story:	1
strangely	1
string	2
sub-processes	1
subcommands.	1
submission	2
submit	3
submitted	4
submitting	1
subshell	1
substitution	1
such	5
suites	1
summary	5
super-users	1
supply	1
support	3
supported	1
supported.	1
supports	1
supportsparse	1
symlink	1
syntax	1
syntax:	1
system	6
system.	1
tag	4
tag.	1
tags	5
taken	2
taken.	1
target	1
tasks	2
tasktracker.	1
template	2
temporary	3
than	4
that	33
the	503
their	2
them	4
then	12
there	1
therefore	20
these	3
things	2
third-party	1
this	118
threads	2
time	3
timeline	3
timelineserver.	1
timestamp.	1
tmpslvnames	1
tmpslvnames=$(echo	1
to	262
too	1
top	2
tr	1
transfer	4
treated	2
tricky.	1
true.	3
turn	1
two	4
type="text/xsl"	6
typical	1
typically	1
u:%user:%user	1
uncomment	3
uncommented	1
uncommenting	1
under	97
underbelly.	1
unique	3
unit	3
units	5
untouched,	1
up	8
updating	1
upon	5
usage	2
use	41
use,	1
use.	4
used	59
used,	1
used.	4
user	58
user-defined	2
user.	2
user1,user2	2
user?	1
users	34
users,	1
users,wheel".	21
uses	7
using	9
utilize	1
utilized.	1
utilizing	1
v2	1
valid.	1
value	50
value,	2
value="20"/>	1
value="30"/>	1
values	5
var:	1
variable	5
variable.	3
variables	5
variables.	1
vast	1
version	2
version="1.0"	5
version="1.0">	1
version="1.0"?>	8
versions.	1
very	1
via	8
view,	1
viewing	2
volume-drivers	1
volumes	2
vs	1
wait	1
want	5
wants	2
warnings	1
warnings.	1
way	1
we	2
we'll	1
weak	1
web	1
well	1
when	27
where	3
which	16
while	4
who	8
why....	1
wildcards	1
will	66
with	75
within	5
work	15
workers	1
workers.sh,	1
would	2
writing	1
writing,	27
written	1
xargs	4
xmlns:xsl="http://www.w3.org/1999/XSL/Transform"	1
xxx-env.sh.	1
yarn	3
yarn-env.sh	1
yarn-service	1
yarn.ewma.cleanupInterval=300	1
yarn.ewma.maxUniqueMessages=250	1
yarn.ewma.messageAgeLimitSeconds=86400	1
yarn.nodemanager.linux-container-executor.group	1
yarn.nodemanager.linux-container-executor.group=#configured	1
yarn.server.resourcemanager.appsummary.log.file	1
yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log	1
yarn.server.resourcemanager.appsummary.logger	2
yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}	1
yet/still,	1
you	38
you.	1
zero	2
zookeeper	1
{YARN_xyz|HDFS_xyz}	1
{yarn-env.sh|hdfs-env.sh}	1
{}	2
|	2
